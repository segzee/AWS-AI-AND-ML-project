# Report: Predict Bike Sharing Demand with AutoGluon Solution


```python
Abiola segun
```

## Initial Training


```python
### Three distinct experiments were conducted to enhance the model's performance:

The first experiment involved submitting the initial raw model as the baseline.

In the second experiment, exploratory data analysis (EDA) was performed, and additional features were incorporated to improve the model's performance.

The third experiment focused on hyperparameter and parameter tuning, facilitated by the AutoGluon tool, to further enhance the model's performance. The AutoGluon framework simplified and streamlined the process, making it more efficient.

During the submission of predictions obtained from these three experiments, it was observed that a few of the experiments resulted in negative prediction values.

To comply with the Kaggle platform's policy, which does not accept submissions with negative predictions, all negative outputs generated by the respective predictors were replaced with a value of 0 before final submission.




What did you realize when you tried to submit your predictions? What changes were needed to the output of the predictor to submit your results?

Three different experiment was carried out
1. The first raw Submission [which consist of the initial model].
2. In order to improve  model  performance exploratory data analysis [EDA] was done additional feature was included.
3. Hyperparameter  and parameter turning was done to help improve the model performance[ with the help of AutoGluon it was so easy and straight to do]

Observation: During the process of submitting predictions obtained from the three experiments, it was noted that a few of the experiments yielded negative prediction values.
Changes incorporated: The Kaggle platform has implemented a policy where submissions containing negative prediction values obtained from the predictor are not accepted. As a result, any negative outputs produced by the respective predictors were replaced with a value of 0 before submission.
```


```python
### What was the top ranked model that performed?

The highest-performing model in this project was the WeightedEnsemble_L3 model, which incorporated additional features. It achieved a validation root mean squared error (RMSE) score of 39.006276 and the best Kaggle score of 0.44931 on the test dataset. This model was developed by training on data that underwent exploratory data analysis (EDA) and feature engineering, without employing a hyperparameter optimization routine. While some models did exhibit improved RMSE scores on the validation data after hyperparameter optimization, the WeightedEnsemble_L3 model demonstrated the best performance on previously unseen test data. It is important to note that several models delivered competitive performance, and the selection process took into account both the RMSE scores from cross-validation and the Kaggle scores from the test data.

It is worth mentioning that the autogluon package employs a peculiar convention where the RMSE scores in the Jupyter notebook are considered negative for the purpose of ranking them in descending order. To obtain the accurate RMSE scores, these negative values need to be multiplied by a factor of '-1'. Therefore, when viewing the results in the Jupyter notebook, the RMSE values may appear as negative numbers due to this convention followed by autogluon.
```


```python
### Exploratory data analysis and feature creation
```


```python
### What did the exploratory analysis find and how did you add additional features?

To extract hour information from the datetime feature, the feature was converted into a datetime format.

Initially, the independent features "season" and "weather" were represented as integers. However, since these variables are categorical in nature, they were transformed into the appropriate category data type.

By performing feature extraction, distinct independent features for year, month, day (dayofweek), and hour were obtained from the datetime feature. Subsequently, the datetime feature itself was dropped from the dataset.

Upon careful analysis, it was observed that the inclusion of the features "casual" and "registered" led to significant improvements in RMSE scores during cross-validation. These features exhibited a strong correlation with the target variable, "count." However, since these features were present only in the train dataset and absent in the test data, they were disregarded during model training.

In order to capture the distinction between weekdays, weekends, and holidays, a new categorical feature called "day_type" was created based on the "holiday" and "workingday" features.

Furthermore, the features "temp" (temperature in degrees Celsius) and "atemp" (feels like temperature in degrees Celsius) displayed a high positive correlation of 0.98. To mitigate multicollinearity issues among the independent variables, "atemp" was removed from both the train and test datasets.

Moreover, data visualization techniques were employed to gain valuable insights from the features.
```


```python
### How much better did your model perform after adding additional features and why do you think that is?

The performance of the model experienced an enhancement when specific categorical variables, initially represented as integer data types, were transformed into their appropriate categorical data types.

Furthermore, to mitigate multicollinearity, not only were the casual and registered features disregarded during model training, but the variable "atemp" was also eliminated from the datasets. This decision was made due to its high correlation with another independent variable, "temp."

Moreover, by splitting the datetime feature into multiple independent features such as year, month, day, hour, and incorporating a new variable called "day_type," the model's performance was further improved. These additional predictor variables enabled the model to effectively assess seasonality and historical patterns in the data.
```


```python
### Hyperparameter tuning
```


```python
### How much better did your model preform after trying different hyper parameters?
Hyperparameter tuning was beneficial because it enhanced the model's performance compared to the initial submission. Three different configurations were used while performing hyperparameter optimization experiments. Although hyperparameter tuned models performance exceptionally well incpmpare to the initial model, the latter ["added feature"] performed exceptionally better on the Kaggle (test) dataset.

Observations:

The utilization of the autogluon package for training took into consideration the specified settings. However, the performance of the hyperparameter-optimized models was not optimal due to the limited options available for autogluon to explore, as the hyperparameters were tuned using a fixed set of values provided by the user.

Furthermore, when conducting hyperparameter optimization with autogluon, the parameters 'time_limit' and 'presets' played a crucial role.

In cases where the time limit was insufficient for model construction, autogluon might fail to build any models for the given set of hyperparameters to be tuned.

Additionally, hyperparameter optimization with presets such as "high_quality" (with auto_stack enabled) demanded high memory usage and imposed computational intensity within the specified time limit and available resources. As a result, lighter and faster preset options like 'medium_quality' and 'optimized_for_deployment' were explored. Among these, I favored the faster and lighter preset, "optimized_for_deployment," for the hyperparameter optimization routine, as the other presets failed to generate models using AutoGluon for the experimental configurations.

One of the significant challenges encountered when employing AutoGluon with a specified range of hyperparameters is striking the right balance between exploration and exploitation.
```


```python
### If you were given more time with this dataset, where do you think you would spend more time?
Given more time to work with this dataset, I would like to investigate additional potential outcomes when AutoGluon is run for an extended period with a high quality preset and enhanced hyperparameter tuning

```


```python
### Create a table with the models you ran, the hyperparameters modified, and the kaggle score.

|model|hpo1|hpo2|hpo3|score|
|initial|prescribed_values|prescribed_values|"presets: 'high quality' (auto_stack=True)"|1.84484|
|add_features|prescribed_value|prescribed_value|"presets: 'high quality' (auto_stack=True)"|0.44931|
|hpo(top-hpo-model: hpo2)|Tree-Based Models: (GBM, XT, XGB & RF)|Tree-Based Models: (GBM, XT, XGB & RF)|"presets: 'optimize_for_deployment"	|0.51677|
```


```python
### Create a line plot showing the top model score for the three (or more) trainingruns during the project

TODO: Replace the image below with your own.

![model_train_score.png](img/model_train_score.png)
```


```python
### Create a line plot showing the top kaggle score for the three (or more) prediction submissions during the project.
![model_test_score.png](img/model_test_score.png)
```


```python

```

# Summary


The AutoGluon AutoML framework for Tabular Data was thoroughly studied and incorporated into this bike sharing demand prediction project.

The project made full use of the capabilities offered by the AutoGluon framework to create stack ensembles and individually configured regression models. These models were trained on tabular data, allowing for the rapid development of a baseline model.

The top-performing model in this project was based on AutoGluon and demonstrated significant improvements. It leveraged data obtained from extensive exploratory data analysis (EDA) and feature engineering, without the need for hyperparameter optimization.

AutoGluon's automatic hyperparameter tuning, model selection/ensembling, and architecture search were instrumental in exploring and exploiting the best possible options for the model.

Moreover, employing hyperparameter tuning with AutoGluon resulted in improved performance compared to the initial raw submission. However, it fell short of the model that incorporated EDA, feature engineering, and had no hyperparameter tuning.

It was observed that hyperparameter tuning with AutoGluon, especially without default hyperparameters or random configuration of parameters, could be a complex process. The effectiveness of the tuning was highly dependent on factors such as the time limit, prescribed presets, possible model families, and the range of hyperparameters to be tuned.
